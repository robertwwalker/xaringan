---
title: "Choice and Forecasting: Week 10"
author: "Robert W. Walker"
format: 
   revealjs:
     theme: [custom.scss]
     scrollable: true
     logo: AGSMlogo.jpeg
     footer: "Models of Choice and Forecasting"
     self-contained: true
     html-math-method: katex
     incremental: true
     slide-number: c/t
     transition: convex
     multiplex: true
     preview-links: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE, echo=TRUE, tidy=TRUE, comment=NA, prompt=FALSE, fig.height=6, fig.width=6.5, fig.retina = 3, dev = 'svg', eval=TRUE)
library(tidyverse)
library(patchwork)
library(gganimate)
options(
  digits = 3,
  width = 75,
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis",
  ggplot2.discrete.colour = c("#D55E00", "#0072B2", "#009E73", "#CC79A7", "#E69F00", "#56B4E9", "#F0E442"),
  ggplot2.discrete.fill = c("#D55E00", "#0072B2", "#009E73", "#CC79A7", "#E69F00", "#56B4E9", "#F0E442")
)
library(fpp3)
austa <- readr::read_csv("http://OTexts.com/fpp3/extrafiles/austa.csv") %>%
  as_tsibble(index = Year)
melsyd <- tsibbledata::ansett %>%
  filter(Airports == "MEL-SYD")
global_economy <- global_economy %>%
  select(Year, Country, GDP, Imports, Exports, Population)
tourism <- tourism %>%
  mutate(
    State = recode(State,
      "Australian Capital Territory" = "ACT",
      "New South Wales" = "NSW",
      "Northern Territory" = "NT",
      "Queensland" = "QLD",
      "South Australia" = "SA",
      "Tasmania" = "TAS",
      "Victoria" = "VIC",
      "Western Australia" = "WA"
    )
  )
beer <- aus_production %>%
  select(Quarter, Beer) %>%
  filter(year(Quarter) >= 1992)
```


# Some Data for Today and General Considerations

Panel data.  Multiple time series are often described as a panel, a cross-section of time series, or a time series of cross-sections.  The data structure has two [non-overlapping] indices.  Let's review, and discuss a bit, what exactly we mean.

## Extending the Data

`fredr` is amazing.

---

```
US.Employment <- map_dfr(
c(rownames(table(us_employment$Series_ID))), ~fredr::fredr_series_observations(.))
save(US.Employment, file="USEmployment.RData")
load(url("https://github.com/robertwwalker/xaringan/raw/master/CMF-Week-9/USEmployment.RData"))
```

```{r}
load("USEmployment.RData")
us_employment %>% data.frame() %>% group_by(Series_ID) %>% summarise(Title = first(Title)) %>% mutate(series_id = Series_ID) %>% ungroup() %>% select(-Series_ID) -> Names.List
US.Employment.T <- left_join(US.Employment, Names.List, by = c("series_id" = "series_id")) %>% mutate(YM = yearmonth(date)) %>% rename(Employed = value) %>% as_tsibble(., index=YM, key=Title)
```

## Additional Features

For much of the study of time series, the key issue is one known as stationarity.  For now, we will do at least some hand waving to be clarified in chapters 5 and more in 9.  But we want to compute things and then build out all the details.  Let's take my new retail employment data.

# A Recreation on New Data

```{r, eval=FALSE}
EMPN <- US.Employment.T %>% filter(YM > yearmonth("1990-01") & Title=="Retail Trade") %>% as_tsibble(index=YM) 
EMPO <- us_employment %>% filter(Title=="Retail Trade" & Month > yearmonth("1990-01")) %>% as_tsibble(., index=Month) 
Plot1 <- ggplot(EMPN, aes(x=YM, y=Employed)) + geom_line(color = "red") + geom_line(data=EMPO, aes(x=Month, y=Employed), inherit.aes=FALSE)
Plot1
```


## Data are Revised Occasionally

```{r P1A, echo=FALSE}
EMPN <- US.Employment.T %>% filter(YM > yearmonth("1990-01") & Title=="Retail Trade") %>% as_tsibble(index=YM) 
EMPO <- us_employment %>% filter(Title=="Retail Trade" & Month > yearmonth("1990-01")) %>% as_tsibble(., index=Month) 
Plot1 <- ggplot(EMPN, aes(x=YM, y=Employed)) + geom_line(color = "red") + geom_line(data=EMPO, aes(x=Month, y=Employed), inherit.aes=FALSE)
Plot1
```


---
  
```{r, eval=FALSE}
library(patchwork)
dcmp <- EMPO %>%
  model(stl = STL(Employed))
Plot2 <- components(dcmp) %>% autoplot()
dcmp <- EMPN %>%
  model(stl = STL(Employed))
Plot3 <- components(dcmp) %>% autoplot()
Plot1 / (Plot2 + Plot3)
```

---
  
```{r P2, echo=FALSE, fig.height=8, fig.width=11}
library(patchwork)
dcmp <- EMPO %>%
  model(stl = STL(Employed))
Plot2 <- components(dcmp) %>% autoplot()
dcmp <- EMPN %>%
  model(stl = STL(Employed))
Plot3 <- components(dcmp) %>% autoplot()
Plot1 / (Plot2 + Plot3)
```


# Three Sectors
  
```{r}
USET <- US.Employment.T %>% filter(YM > yearmonth("1990-01"), Title%in%c("Retail Trade","Financial Activities","Manufacturing")) %>% as_tsibble(., index=YM, key=Title) 
USET %>% autoplot(Employed)
```

## Retail (season)
  
```{r}
US.Employment.T %>% 
  filter(YM > yearmonth("1990-01"), 
         Title%in%c("Retail Trade")) %>% 
  as_tsibble(., index=YM) %>% 
  gg_season(Employed)
```

## Retail (subseries)
  
```{r}
US.Employment.T %>% 
  filter(YM > yearmonth("1990-01"), 
         Title%in%c("Retail Trade")) %>% 
  as_tsibble(., index=YM) %>% 
  gg_subseries(Employed)
```

## Retail (lag)
  
```{r}
US.Employment.T %>% 
  filter(YM > yearmonth("1990-01"), 
         Title%in%c("Retail Trade")) %>% 
  as_tsibble(., index=YM) %>% 
  gg_lag(Employed)
```


## Manufacturing
  
```{r}
US.Employment.T %>% filter(YM > yearmonth("1990-01"), Title%in%c("Manufacturing")) %>% as_tsibble(., index=YM) %>% gg_season(Employed)
```

## Manufacturing
  
```{r}
US.Employment.T %>% filter(YM > yearmonth("1990-01"), Title%in%c("Manufacturing")) %>% as_tsibble(., index=YM) %>% gg_subseries(Employed)
```

## Manufacturing
  
```{r}
US.Employment.T %>% filter(YM > yearmonth("1990-01"), Title%in%c("Manufacturing")) %>% as_tsibble(., index=YM) %>% gg_lag(Employed)
```

## Financial
  
```{r}
US.Employment.T %>% filter(YM > yearmonth("1990-01"), Title%in%c("Financial Activities")) %>% as_tsibble(., index=YM) %>% gg_season(Employed)
```

## Financial
  
```{r}
US.Employment.T %>% filter(YM > yearmonth("1990-01"), Title%in%c("Financial Activities")) %>% as_tsibble(., index=YM) %>% gg_subseries(Employed)
```

## Financial
  
```{r}
US.Employment.T %>% filter(YM > yearmonth("1990-01"), Title%in%c("Financial Activities")) %>% as_tsibble(., index=YM) %>% gg_lag(Employed)
```


## Features: Summary
  
  
The features command is the magic tool for tidy summary and statistics for time series in this index/key format.  For example, basic summary

```{r}
USET %>% features(Employed, features=list(mean=mean,min=min,max=max,sd=sd,quantile))
```

## Features: Correlation Features
  
Learning about the time series properties

```{r}
USET %>% features(Employed, features=feat_acf)
```
```{r}
USET %>% group_by(Title) %>% ACF(Employed) %>% autoplot()
```

## For Contrast: Ford Returns
  
```{r}
library(tidyquant)
Ford <- tq_get("F", from="2000-01-01")
FordT <- Ford %>% as_tsibble(index=date)
FordT %>% autoplot(adjusted)
```


```{r}
FC <- Ford %>% tq_transmute(adjusted, mutate_fun = periodReturn, period = "monthly") %>% mutate(YM = yearmonth(date)) %>% as_tsibble(., index=YM)
FC %>% autoplot(monthly.returns)
```

## Ford's ACF
  
The 6/7 and 12/13 patterns are interesting....

```{r}
library(patchwork)
FC1 <- FC %>% ACF(monthly.returns) %>% autoplot()
FC2 <- FC %>% PACF(monthly.returns) %>% autoplot()
FC1+FC2
```

## Decomposition Features
  
```{r}
USET %>% features(Employed, feat_stl)
```

## With More Data
  
```{r}
NUSET8k <- US.Employment.T %>% data.frame() %>% group_by(Title) %>% summarise(MaxE = max(Employed)) %>% arrange(desc(MaxE)) %>% filter(MaxE > 8000 & MaxE < 120000) 
USET8k <- left_join(NUSET8k, US.Employment.T) %>% as_tsibble(., index=YM, key=Title)
```

## An Improvement on the Trend/Season
  
```{r, echo=FALSE}
library(plotly); library(widgetframe)
USET8k %>%
  features(Employed, feat_stl) %>%
  ggplot(aes(x = trend_strength, y = seasonal_strength_year, text = Title)) +
  geom_point() -> jj
k <- ggplotly(jj)
k
```

## Features

:::: {.columns}

::: {.column width="50%"}
![Features 1](./img/Screen Shot 2022-11-07 at 3.46.47 PM.png)
:::

::: {.column width="50%"}
![Features 2](./img/Screen Shot 2022-11-07 at 3.47.06 PM.png)
:::

::::




---

The details are at the bottom [for other statistics](https://otexts.com/fpp3/stlfeatures.html).

```{r}
library(kableExtra)
USET8k %>%
  features(Employed, feat_stl) %>% knitr::kable(format="html") %>% scroll_box(width = "100%", height = "300px")
```


## `coef_hurst`
  
A measure of the degree to which adjacent observations depend on one another over time.  Generically, this statistic takes values between zero and one with one indicating very high levels of dependence through time.

```{r}
USET %>% features(Employed, coef_hurst)
```


## Middling for Ford
  
```{r}
FC %>% features(monthly.returns, features=coef_hurst)
```

## `feat_spectral`
  
```{r}
USET %>% features(Employed, feat_spectral)
FC %>% features(monthly.returns, features=feat_spectral)
```

# The Absence of Correlation
  
Ljung-Box modifies the idea in the Box-Pierce statistic for assessing whether or not a given series [or transformation thereof] is essentially uncorrelated.  In both cases, we will get to the details next week [chapter 5].  For now, the idea is simply that $k$ squared autocorrelations will sum to a chi-squared distribution with $k$ degrees of freedom.  Large correlations reveal dependence.

```{r}
USET %>% features(Employed, features=list(box_pierce, ljung_box))
FC %>% features(monthly.returns, features=list(box_pierce, ljung_box))
```


## `feat_pacf`
  
```{r}
USET %>% features(Employed, feat_pacf)
FC %>% features(monthly.returns, features=feat_pacf)
```

## Unit Roots
  
The stationarity issue from earlier is given much attention.  Can we reasonably think of characteristics as fixed?  There are three means of assessment with details to Chapter 9.

```{r}
USET %>% features(Employed, features=list(unitroot_kpss, unitroot_pp, unitroot_ndiffs, unitroot_nsdiffs)) %>% knitr::kable(format="html")
FC %>% features(monthly.returns, features=list(unitroot_kpss, unitroot_pp, unitroot_ndiffs, unitroot_nsdiffs))
```

# Tiling
  
[A reminder](https://davisvaughan.github.io/slider/)

```{r}
USET %>% features(Employed, features=list(var_tiled_mean, var_tiled_var))
FC %>% features(monthly.returns, features=list(var_tiled_mean, var_tiled_var))
```

## Detecting Shifts
  
```{r}
USET %>% features(Employed, features=list(shift_level_max, shift_var_max, shift_kl_max)) %>% kable(format="html")
FC %>% features(monthly.returns, features=list(shift_level_max, shift_var_max, shift_kl_max)) %>% kable(format="html")
```

## Crossings and Flat Spots
  
```{r}
USET %>% features(Employed, features=list(n_crossing_points, longest_flat_spot)) %>% kable(format="html")
FC %>% features(monthly.returns, features=list(n_crossing_points, longest_flat_spot)) %>% kable(format="html")
```

## ARCH
  
What proportion of the current squared residual is explained by the prior squared residual?  This reports $R^2$; if the variance explained is large, volatility is persistent.  **There is a chi-square statistic also.**
  
```{r}
USET %>% features(Employed, features=stat_arch_lm) %>% kable(format="html")
FC %>% features(monthly.returns, features=stat_arch_lm) %>% kable(format="html")
```

## The Box-Cox
  
```{r}
USET %>% features(Employed, features=guerrero) %>% kable(format="html")
FC %>% features(monthly.returns, features=guerrero) %>% kable(format="html")
```



```{r}
USET %>% features(Employed, features=guerrero)
```

## Filtered Manufacturing
  
```{r}
USET %>% filter(Title=="Manufacturing") %>% mutate(Filt = box_cox(Employed, 1.0369662)) %>% select(YM,Filt,Employed) %>% pivot_longer(c(Filt,Employed)) %>% autoplot(value)
```


```{r}
USET %>% filter(Title=="Financial Activities") %>% autoplot(box_cox(Employed, 0.9481456))
```

```{r}
USET %>% filter(Title=="Retail Trade") %>% autoplot(box_cox(Employed, 1.1860464))
```

```{r}
FC %>% features(monthly.returns, features=guerrero)
FC %>% autoplot(box_cox(monthly.returns, 0.6857523))
```

# Australian Tourism
  
[The example is great.](https://otexts.com/fpp3/exploring-australian-tourism-data.html)


# Principal Components
  
Lets walk through this example.

![Ex. PC](./img/Screen Shot 2022-10-31 at 3.09.12 PM.png)

## Principal Components

```{r}
tourism_features <- tourism %>%
  features(Trips, feature_set(pkgs = "feasts"))
library(broom)
pcs <- tourism_features %>%
  select(-State, -Region, -Purpose) %>%
  prcomp(scale = TRUE) %>%
  augment(tourism_features)
pcs %>%
  ggplot(aes(x = .fittedPC1, y = .fittedPC2, col = Purpose)) +
  geom_point() +
  theme(aspect.ratio = 1)
```


## The Forecasting Workflow

![Diagram](./img/Screen Shot 2022-11-07 at 12.51.46 PM.png)

## Four Simple Methods

- `MEAN()` or the mean method
- `NAIVE()` or naive
- `SNAIVE()` or seasonal naive
- `RW()` or random walk/drift

## An Example

```{r, eval=FALSE}
# Set training data from 1992 to 2006
train <- aus_production %>%
  filter_index("1992 Q1" ~ "2006 Q4")
# Fit the models
beer_fit <- train %>%
  model(
    Mean = MEAN(Beer),
    `Naïve` = NAIVE(Beer),
    `Seasonal naïve` = SNAIVE(Beer)
  )
# Generate forecasts for 14 quarters
beer_fc <- beer_fit %>% forecast(h = 14)
# Plot forecasts against actual values
beer_fc %>%
  autoplot(train, level = NULL) +
  autolayer(
    filter_index(aus_production, "2007 Q1" ~ .),
    colour = "black"
  ) +
  labs(
    y = "Megalitres",
    title = "Forecasts for quarterly beer production"
  ) +
  guides(colour = guide_legend(title = "Forecast"))
```

---

```{r, echo=FALSE}
# Set training data from 1992 to 2006
train <- aus_production %>%
  filter_index("1992 Q1" ~ "2006 Q4")
# Fit the models
beer_fit <- train %>%
  model(
    Mean = MEAN(Beer),
    `Naïve` = NAIVE(Beer),
    `Seasonal naïve` = SNAIVE(Beer)
  )
# Generate forecasts for 14 quarters
beer_fc <- beer_fit %>% forecast(h = 14)
# Plot forecasts against actual values
beer_fc %>%
  autoplot(train, level = NULL) +
  autolayer(
    filter_index(aus_production, "2007 Q1" ~ .),
    colour = "black"
  ) +
  labs(
    y = "Megalitres",
    title = "Forecasts for quarterly beer production"
  ) +
  guides(colour = guide_legend(title = "Forecast"))
```

## A Google stock example

**Note the reindexing.**

```{r}
# Re-index based on trading days
google_stock <- gafa_stock %>%
  filter(Symbol == "GOOG", year(Date) >= 2015) %>%
  mutate(day = row_number()) %>%
  update_tsibble(index = day, regular = TRUE)
# Filter the year of interest
google_2015 <- google_stock %>% filter(year(Date) == 2015)
# Fit the models
google_fit <- google_2015 %>%
  model(
    Mean = MEAN(Close),
    `Naïve` = NAIVE(Close),
    Drift = NAIVE(Close ~ drift())
  )
# Produce forecasts for the trading days in January 2016
google_jan_2016 <- google_stock %>%
  filter(yearmonth(Date) == yearmonth("2016 Jan"))
google_fc <- google_fit %>%
  forecast(new_data = google_jan_2016)
# Plot the forecasts
google_fc %>%
  autoplot(google_2015, level = NULL) +
  autolayer(google_jan_2016, Close, colour = "black") +
  labs(y = "$US",
       title = "Google daily closing stock prices",
       subtitle = "(Jan 2015 - Jan 2016)") +
  guides(colour = guide_legend(title = "Forecast"))
```

## `augment`

Adds three new columns to data.  

- The fitted value, 
- the residual, 
- and the *innovation residual*.  

The latter two will be identical unless some transformation is used, in which case the residual is in the base term [y] while the innovation is in the transformed metric [$w = f(y)$].

## Google stock price residuals

```{r}
google_2015 %>%
  model(NAIVE(Close)) %>%
  gg_tsresiduals()
```

## Two portmanteau tests

![BP Test](./img/Screen Shot 2022-11-07 at 1.12.12 PM.png)

![LBQ Test](./img/Screen Shot 2022-11-07 at 1.13.03 PM.png)

```{r}
aug <- google_2015 %>%
  model(NAIVE(Close)) %>%
  augment()
aug %>% features(.innov, box_pierce, lag = 10, dof = 0)
aug %>% features(.innov, ljung_box, lag = 10, dof = 0)
```

## Uncertainty and Intervals

![Intervals](./img/Screen Shot 2022-11-07 at 1.16.54 PM.png)

```{r}
google_2015 %>%
  model(NAIVE(Close)) %>%
  forecast(h = 10) %>%
  hilo()
```

## Plotting Forecasts

```{r}
google_2015 %>%
  model(NAIVE(Close)) %>%
  forecast(h = 10) %>%
  autoplot(google_2015) +
  labs(title="Google daily closing stock price", y="$US" )
```

## Bootstrap Paths

A bit on the bootstrap.  Using bootstrap intervals is integrated into `fpp3`.

```{r}
fit <- google_2015 %>%
  model(NAIVE(Close))
sim <- fit %>% generate(h = 30, times = 5, bootstrap = TRUE)
sim
```

## Plot them [can automate with forecast]

```{r}
fc <- fit %>% forecast(h = 30, bootstrap = TRUE)
autoplot(fc, google_2015) +
  labs(title="Google daily closing stock price", y="$US" )
```

## Transformations with forecasting

Just like smart prediction, the `fable` package handles the inversion of transformations.  The bigger issue is the need for **bias-adjustment**.

## Decomposition forecasting

The `decomposition_model` function allows for a separate approach to the seasonal and seasonally adjusted components.  The latter is often combined with a model covered later including Holt's method or ARIMA models.

## Two Examples

:::: {.columns}

::: {.column width="50%"}
```{r}
us_retail_employment <- us_employment %>%
  filter(year(Month) >= 1990, Title == "Retail Trade")
fit_dcmp <- us_retail_employment %>%
  model(stlf = decomposition_model(
    STL(Employed ~ trend(window = 7), robust = TRUE),
    NAIVE(season_adjust)
  ))
fit_dcmp %>%
  forecast() %>%
  autoplot(us_retail_employment)+
  labs(y = "Number of people",
       title = "US retail employment")
fit_dcmp %>% gg_tsresiduals()
```
:::

::: {.column width="50%"}
```{r}
fit_dcmp <- us_retail_employment %>%
  model(stlf = decomposition_model(
    STL(Employed ~ trend(window = 7), robust = TRUE),
    RW(season_adjust)
  ))
fit_dcmp %>%
  forecast() %>%
  autoplot(us_retail_employment)+
  labs(y = "Number of people",
       title = "US retail employment")
fit_dcmp %>% gg_tsresiduals()
```
:::

::::

## Evaluating Forecast Accuracy of Points

- On training and test [usually 20%]
   - `filter` and `slice`
- Scale dependent errors
   - MAE: mean absolute error
   - RMSE: root mean squared error
- Percentage errors
   - MAPE: mean absolute percentage error
   - sMAPE: symmetric MAPE
- Scaled errors [def in 5.8]
   - MASE: mean absolute scaled error
   - RMSSE: root mean square scaled error

`accuracy`

## Evaluating Accuracy of Distributions

`accuracy(., list(...))`

- Quantile scores `quantile_score`
- Winkler scores `winkler_score`
- Continuous ranked probability scores `CRPS`
- Skill scores `skill_score()` with a statistic for comparison.

# Time Series Cross-Validation

Multiple training and test sets based on cross-validation ideas.  **NB: Dates are discontinuous.**

```{r}
G22 <- tidyquant::tq_get("GOOG", from="2021-01-01") %>% select(date, symbol, close) %>% mutate(Date = row_number()) %>% as_tsibble(index=Date)
G22_tr <- G22 %>%
  stretch_tsibble(.init = 90, .step = 1) %>%
  relocate(date, symbol, .id)
G22_tr %>%
  model(rw = RW(close ~ drift())) %>%
  forecast(h = 1) %>%
  accuracy(G22)
```

# Judgmental Forecasts

1. Limitations
2. Key principles
3. Delphi method
4. Analogy
5. Scenarios
6. New product forecasting
7. Judgmental adjustments

## Key Principles

- Set the forecasting task clearly and concisely
- Implement a systematic approach
- Document and justify
- Systematically evaluate forecasts
- Segregate forecasters and users

## Delphi

![Delphi](./img/Screen Shot 2022-11-07 at 3.18.47 PM.png)

## Analogies

![Structured Analogies](./img/Screen Shot 2022-11-07 at 3.19.40 PM.png)

## Adjustments

- Use them sparingly.
- Make them structured.

# The Path Forward: Main Models

1. Develop time series regression models
2. Exponential smoothing
3. ARIMA models
4. Dynamic regression models

- Hierarchies and groups
- Advanced methods
- Practical issues